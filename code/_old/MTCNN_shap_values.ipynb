{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from facenet_pytorch import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = pickle.load( open( \"../data/not_found.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=(218,178), margin=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be used in the explaienr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[60.075836181640625 69.58525085449219 155.28472900390625\n",
      "  181.38697814941406]] [0.9999779462814331]\n"
     ]
    }
   ],
   "source": [
    "# Open an image\n",
    "img_path = not_found[0]\n",
    "\n",
    "pic = Image.open(img_path)\n",
    "pic = np.array(pic)\n",
    "\n",
    "boxes, conf = mtcnn.detect(pic)\n",
    "\n",
    "print(\"Prediction:\", boxes, conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (200, 10) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlime\u001b[39;00m \u001b[39mimport\u001b[39;00m lime_image\n\u001b[1;32m      3\u001b[0m explainer \u001b[39m=\u001b[39m lime_image\u001b[39m.\u001b[39mLimeImageExplainer()\n\u001b[0;32m----> 5\u001b[0m explanation \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mexplain_instance(pic\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mdouble\u001b[39;49m\u001b[39m'\u001b[39;49m), mtcnn\u001b[39m.\u001b[39;49mdetect, top_labels\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, hide_color\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/envs/fairness/lib/python3.11/site-packages/lime/lime_image.py:198\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    194\u001b[0m     fudged_image[:] \u001b[39m=\u001b[39m hide_color\n\u001b[1;32m    196\u001b[0m top \u001b[39m=\u001b[39m labels\n\u001b[0;32m--> 198\u001b[0m data, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_labels(image, fudged_image, segments,\n\u001b[1;32m    199\u001b[0m                                 classifier_fn, num_samples,\n\u001b[1;32m    200\u001b[0m                                 batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m    202\u001b[0m distances \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mpairwise_distances(\n\u001b[1;32m    203\u001b[0m     data,\n\u001b[1;32m    204\u001b[0m     data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    205\u001b[0m     metric\u001b[39m=\u001b[39mdistance_metric\n\u001b[1;32m    206\u001b[0m )\u001b[39m.\u001b[39mravel()\n\u001b[1;32m    208\u001b[0m ret_exp \u001b[39m=\u001b[39m ImageExplanation(image, segments)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/envs/fairness/lib/python3.11/site-packages/lime/lime_image.py:267\u001b[0m, in \u001b[0;36mLimeImageExplainer.data_labels\u001b[0;34m(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    265\u001b[0m     preds \u001b[39m=\u001b[39m classifier_fn(np\u001b[39m.\u001b[39marray(imgs))\n\u001b[1;32m    266\u001b[0m     labels\u001b[39m.\u001b[39mextend(preds)\n\u001b[0;32m--> 267\u001b[0m \u001b[39mreturn\u001b[39;00m data, np\u001b[39m.\u001b[39marray(labels)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (200, 10) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from lime import lime_image\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "explanation = explainer.explain_instance(pic.astype('double'), mtcnn.detect, top_labels=5, hide_color=0, num_samples=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[60.075836181640625 69.58525085449219 155.28472900390625\n",
      "  181.38697814941406]] [0.9999779462814331]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "<class 'method'> is not currently a supported model type!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction:\u001b[39m\u001b[39m\"\u001b[39m, boxes, conf)\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mGradientExplainer(mtcnn\u001b[39m.\u001b[39;49mdetect, pic)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/envs/fairness/lib/python3.11/site-packages/shap/explainers/_gradient.py:75\u001b[0m, in \u001b[0;36mGradient.__init__\u001b[0;34m(self, model, data, session, batch_size, local_smoothing)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m _TFGradient(model, data, session, batch_size, local_smoothing)\n\u001b[1;32m     76\u001b[0m \u001b[39melif\u001b[39;00m framework \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m _PyTorchGradient(model, data, batch_size, local_smoothing)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/envs/fairness/lib/python3.11/site-packages/shap/explainers/_gradient.py:159\u001b[0m, in \u001b[0;36m_TFGradient.__init__\u001b[0;34m(self, model, data, session, batch_size, local_smoothing)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m# determine the model inputs and outputs\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs \u001b[39m=\u001b[39m _get_model_inputs(model)\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m _get_model_output(model)\n\u001b[1;32m    161\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mThe model output to be explained must be a single tensor!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/envs/fairness/lib/python3.11/site-packages/shap/explainers/tf_utils.py:68\u001b[0m, in \u001b[0;36m_get_model_inputs\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m model[\u001b[39m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(model)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is not currently a supported model type!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: <class 'method'> is not currently a supported model type!"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import shap\n",
    "explainer = shap.GradientExplainer(mtcnn.detect, pic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alg_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
